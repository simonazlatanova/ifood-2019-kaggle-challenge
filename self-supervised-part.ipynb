{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13663,"databundleVersionId":401923,"sourceType":"competition"},{"sourceId":12219365,"sourceType":"datasetVersion","datasetId":7698345},{"sourceId":12219446,"sourceType":"datasetVersion","datasetId":7698400},{"sourceId":441859,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":359080,"modelId":380393}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Supervised Learning - Self Supervised Part\n\n\n---\n\nThis notebook contains our implementation of the self-supervised learning (SSL) part of the project, based on the SimCLR framework. We use a custom CNN encoder trained without labels, and evaluate the learned representations by training various traditional classifiers on top of extracted features.\n\nThe notebook covers the full SSL pipeline, from data augmentation and model setup to evaluation with logistic regression, SVM, random forest, and a neural network head.\n\n---\n\n## 1. Data Preparation\n- Unzip and organize the dataset structure\n- Define `SSLFoodDataset` for generating augmented view pairs\n- Apply SimCLR-style data augmentations\n- Compute and use dataset-specific normalization stats\n\n## 2. Model Architecture\n- Define a ResNet18-inspired CNN (`CustomCNN`) with CBAM\n- Wrap it in a `SimCLR` model with a projection head\n- Implement NT-Xent contrastive loss\n\n##  3. SSL Pretraining (SimCLR)\n- Train the SimCLR model on the training set (20 or 35 epochs)\n- Use frozen encoder, no labels involved\n- Periodically save model checkpoints\n\n## 4. Feature Extraction\n- Use frozen encoder to extract 256-dimensional vectors\n- Generate train/val/test sets for downstream classifiers\n- Save feature arrays for reuse\n\n## 5. Training Traditional Classifiers\n- Train and evaluate:\n  - Logistic Regression\n  - Linear SVM (full + val-only)\n  - RBF SVM (subset only)\n  - Random Forest\n  - MLP (PyTorch)\n\n## 6. Evaluation & Analysis\n- Compare classifier performance on extracted SSL features\n- Analyze effects of data volume, label quality, and model type\n- Reflect on constraints and generalization\n\n","metadata":{}},{"cell_type":"code","source":"#pip install tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:08:45.657605Z","iopub.execute_input":"2025-06-18T18:08:45.657869Z","iopub.status.idle":"2025-06-18T18:08:45.661389Z","shell.execute_reply.started":"2025-06-18T18:08:45.657849Z","shell.execute_reply":"2025-06-18T18:08:45.660601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import libraries\nimport os\n# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nimport shutil\nimport zipfile\nimport random\nimport time\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport torch\n# torch.cuda.empty_cache()\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms\nimport multiprocessing\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T23:07:24.720382Z","iopub.execute_input":"2025-06-19T23:07:24.720555Z","iopub.status.idle":"2025-06-19T23:07:35.077737Z","shell.execute_reply.started":"2025-06-19T23:07:24.720538Z","shell.execute_reply":"2025-06-19T23:07:35.077155Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# --- for reproducibility ---\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef seed_worker(worker_id):\n    seed = torch.initial_seed() % 2**32\n    np.random.seed(seed)\n    random.seed(seed)\n    \nset_seed(42)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T23:07:35.080925Z","iopub.execute_input":"2025-06-19T23:07:35.081643Z","iopub.status.idle":"2025-06-19T23:07:35.090440Z","shell.execute_reply.started":"2025-06-19T23:07:35.081620Z","shell.execute_reply":"2025-06-19T23:07:35.089911Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"---\n\n\n# General Data Loader","metadata":{}},{"cell_type":"code","source":"data_dir = Path('/kaggle/input/ifood-2019-fgvc6')\nworking_dir = Path('/kaggle/working')\nTRAIN_CSV = data_dir / 'train_labels.csv'\n\n# Extract and flatten function\ndef extract_and_flatten(zip_filename, folder_name, internal_folder_name):\n    zip_path = data_dir / zip_filename\n    extract_path = working_dir  / (folder_name + \"_temp\")\n    final_path = working_dir / folder_name\n\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        zip_ref.extractall(extract_path)\n\n    nested = extract_path / internal_folder_name\n    final_path.mkdir(exist_ok=True)\n    for fname in os.listdir(nested):\n        shutil.move(str(nested / fname), str(final_path / fname))\n\n    shutil.rmtree(extract_path)\n\n\n# Unpack all datasets\nextract_and_flatten(\"train_set.zip\", \"train\", \"train_set\")\nextract_and_flatten(\"val_set.zip\", \"val\", \"val_set\")\nextract_and_flatten(\"test_set.zip\", \"test\", \"test_set\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:23:18.126210Z","iopub.execute_input":"2025-06-19T15:23:18.126646Z","iopub.status.idle":"2025-06-19T15:24:27.911146Z","shell.execute_reply.started":"2025-06-19T15:23:18.126607Z","shell.execute_reply":"2025-06-19T15:24:27.910102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Custom Dataset Class for our Purposes ---\nclass FoodDataset(Dataset):\n    def __init__(self, image_dir, labels_df, transform, class_to_idx):\n        self.image_dir = image_dir\n        self.labels_df = labels_df\n        self.transform = transform\n        self.class_to_idx = class_to_idx\n\n    def __len__(self):\n        return len(self.labels_df)\n\n    def __getitem__(self, idx):\n        row = self.labels_df.iloc[idx]\n        img_path = os.path.join(self.image_dir, row['img_name'])\n        image = Image.open(img_path).convert('RGB')\n        label = self.class_to_idx[row['label']]\n\n        image = self.transform(image)\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:27.919336Z","iopub.execute_input":"2025-06-19T15:24:27.919691Z","iopub.status.idle":"2025-06-19T15:24:27.938170Z","shell.execute_reply.started":"2025-06-19T15:24:27.919661Z","shell.execute_reply":"2025-06-19T15:24:27.936969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Compute dataset-specific mean and std ---\ndef compute_mean_std(image_dir):\n    \"\"\"\n    Calculates mean and standard deviation across all RGB images.\n    \"\"\"\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor()\n    ])\n\n    image_paths = list(Path(image_dir).glob(\"*.jpg\"))\n\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    total_images = 0\n\n    for img_path in tqdm(image_paths, desc=\"Computing mean/std\"):\n        image = Image.open(img_path).convert(\"RGB\")\n        tensor = transform(image)\n        mean += tensor.mean(dim=(1, 2))\n        std += tensor.std(dim=(1, 2))\n        total_images += 1\n\n    mean /= total_images\n    std /= total_images\n\n    print(\"Mean:\", mean.tolist())\n    print(\"Std:\", std.tolist())\n    return mean.tolist(), std.tolist()\n\n# Already computed → no need to run again\n# compute_mean_std(working_dir / 'train')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:27.939357Z","iopub.execute_input":"2025-06-19T15:24:27.939657Z","iopub.status.idle":"2025-06-19T15:24:27.961052Z","shell.execute_reply.started":"2025-06-19T15:24:27.939630Z","shell.execute_reply":"2025-06-19T15:24:27.959979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Transforms for supervised and self-supervised training ---\n\n# Augmented transform for supervised training\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.6388, 0.5444, 0.4448], std=[0.2229, 0.2414, 0.2638])\n])\n\n# Validation/test transform (no augmentation)\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.6388, 0.5444, 0.4448], std=[0.2229, 0.2414, 0.2638])\n])\n\n# SimCLR-style augmentations for SSL\nsimclr_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n    transforms.RandomGrayscale(p=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.6388, 0.5444, 0.4448],\n                         std=[0.2229, 0.2414, 0.2638])\n])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:27.962187Z","iopub.execute_input":"2025-06-19T15:24:27.962587Z","iopub.status.idle":"2025-06-19T15:24:27.986133Z","shell.execute_reply.started":"2025-06-19T15:24:27.962560Z","shell.execute_reply":"2025-06-19T15:24:27.985047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Label encoding ---\ntrain_csv = data_dir / 'train_labels.csv'\nlabels_df = pd.read_csv(train_csv)\n\n# Sort class names for consistent indexing\nclasses = sorted(labels_df['label'].unique())\nclass_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n\n# Map string labels to class indices\nlabels = labels_df['label'].map(class_to_idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:28.007018Z","iopub.execute_input":"2025-06-19T15:24:28.007340Z","iopub.status.idle":"2025-06-19T15:24:28.147042Z","shell.execute_reply.started":"2025-06-19T15:24:28.007318Z","shell.execute_reply":"2025-06-19T15:24:28.145912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Stratified Train/Validation Split ---\ntrain_idx, val_idx = train_test_split(\n    np.arange(len(labels_df)),\n    test_size=0.2,\n    stratify=labels,\n    random_state=42\n)\n\n# Create new DataFrames\ntrain_df = labels_df.iloc[train_idx].reset_index(drop=True)\nval_df = labels_df.iloc[val_idx].reset_index(drop=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Dataset Instances with Transforms ---\ntrain_dataset = FoodDataset(\n    working_dir / 'train',\n    train_df,\n    transform=train_transform,\n    class_to_idx=class_to_idx\n)\n\nval_dataset = FoodDataset(\n    working_dir / 'train',\n    val_df,\n    transform=val_transform,\n    class_to_idx=class_to_idx\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- DataLoader Setup ---\n# Adjust number of workers based on system\ncpu_count = multiprocessing.cpu_count()\nnum_workers = 2  # Adjustable; 2 is safe default\n\n# Training Loader\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    num_workers=num_workers,\n    pin_memory=True,\n    persistent_workers=True,\n    worker_init_fn=seed_worker\n)\n\n# Validation Loader\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=64,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True,\n    persistent_workers=True,\n    worker_init_fn=seed_worker\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:28.231612Z","iopub.execute_input":"2025-06-19T15:24:28.231892Z","iopub.status.idle":"2025-06-19T15:24:28.238306Z","shell.execute_reply.started":"2025-06-19T15:24:28.231869Z","shell.execute_reply":"2025-06-19T15:24:28.237305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Test Loader: from official validation set ---\ntest_csv = data_dir / 'val_labels.csv'\ntest_labels_df = pd.read_csv(test_csv).reset_index(drop=True)\n\ntest_dataset = FoodDataset(\n    working_dir / 'val',\n    test_labels_df,\n    transform=val_transform,\n    class_to_idx=class_to_idx\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=64,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True,\n    persistent_workers=True,\n    worker_init_fn=seed_worker\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:28.239264Z","iopub.execute_input":"2025-06-19T15:24:28.239671Z","iopub.status.idle":"2025-06-19T15:24:28.272527Z","shell.execute_reply.started":"2025-06-19T15:24:28.239632Z","shell.execute_reply":"2025-06-19T15:24:28.271491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n# Definition of the original Net\n","metadata":{}},{"cell_type":"code","source":"# --- Custom Residual Block with SiLU Activation ---\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n                               stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.act = nn.SiLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x if self.downsample is None else self.downsample(x)\n        out = self.act(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += identity\n        return self.act(out)\n\n\n# --- CBAM Attention Module (Channel + Spatial Attention) ---\nclass CBAMBlock(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.channel_fc = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n            nn.ReLU(),\n            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n        )\n\n        self.spatial = nn.Sequential(\n            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),\n            nn.Sigmoid()\n        )\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Channel attention\n        avg_out = self.channel_fc(self.avg_pool(x))\n        max_out = self.channel_fc(self.max_pool(x))\n        channel_att = self.sigmoid(avg_out + max_out)\n        x = x * channel_att\n\n        # Spatial attention\n        avg_pool = torch.mean(x, dim=1, keepdim=True)\n        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n        spatial_att = self.spatial(torch.cat([avg_pool, max_pool], dim=1))\n        return x * spatial_att\n\n# --- Custom CNN Definition (ResNet-like with CBAM and SiLU) ---\nclass CustomCNN(nn.Module):\n    def __init__(self, block, layers, num_classes=251, base_width=32):\n        super().__init__()\n        self.in_channels = base_width\n\n        self.conv1 = nn.Conv2d(3, base_width, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(base_width)\n        self.act = nn.SiLU(inplace=True)\n\n        self.layer1 = self._make_layer(block, base_width, layers[0])\n        self.layer2 = self._make_layer(block, base_width*2, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, base_width*4, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, base_width*8, layers[3], stride=2)\n\n        self.cbam = CBAMBlock(base_width * 8)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(0.3)\n\n        self.fc = nn.Sequential(\n            nn.Linear(base_width * 8, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n        self._init_weights()\n\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        \"\"\"\n        Creates a sequence of residual blocks, optionally with downsampling.\n        \"\"\"\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion)\n            )\n\n        layers = [block(self.in_channels, out_channels, stride, downsample)]\n        self.in_channels = out_channels * block.expansion\n        layers.extend([block(self.in_channels, out_channels) for _ in range(1, blocks)])\n        return nn.Sequential(*layers)\n\n\n    def _init_weights(self):\n        \"\"\"\n        Initializes weights using He initialization (Kaiming) for conv layers.\n        \"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        \"\"\"\n        Full forward pass with classification head.\n        \"\"\"\n        x = self.act(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.cbam(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n\n        \n    def extract_features(self, x):\n        \"\"\"\n        Feature extractor for SSL: excludes final classification head.\n        \"\"\"\n        x = self.act(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.cbam(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        return x\n\n\n\n# --- Wrapper function ---\ndef build_custom_cnn(num_classes=251):\n    \"\"\"\n    Builds ResNet18-like model with custom blocks and CBAM.\n    \"\"\"\n    return CustomCNN(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, base_width=32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:28.273549Z","iopub.execute_input":"2025-06-19T15:24:28.273792Z","iopub.status.idle":"2025-06-19T15:24:28.297062Z","shell.execute_reply.started":"2025-06-19T15:24:28.273774Z","shell.execute_reply":"2025-06-19T15:24:28.295964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Inspect Model Summary to check the Number of Parameters (<5M) ---\nfrom torchsummary import summary\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CustomCNN(BasicBlock, [2, 2, 2, 2], num_classes=251, base_width=32).to(device)\n\nsummary(model, input_size=(3, 224, 224), device=str(device))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:28.298038Z","iopub.execute_input":"2025-06-19T15:24:28.298421Z","iopub.status.idle":"2025-06-19T15:24:28.322715Z","shell.execute_reply.started":"2025-06-19T15:24:28.298393Z","shell.execute_reply":"2025-06-19T15:24:28.321726Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n# Self Supervised Learning - SImCLR Model \n\n","metadata":{}},{"cell_type":"code","source":"# --- SimCLR Model: Encoder + Projection Head ---\nclass SimCLR(nn.Module):\n    def __init__(self, base_encoder, projection_dim=128):\n        \"\"\"\n        Wraps a base encoder (e.g., ResNet or custom CNN)\n        and adds a projection head for contrastive learning.\n        \"\"\"\n        super(SimCLR, self).__init__()\n        self.encoder = base_encoder\n        feature_dim = 256  # Output dim from extract_features()\n\n        self.projector = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim),\n            nn.ReLU(),\n            nn.Linear(feature_dim, projection_dim)\n        )\n\n    def forward(self, x):\n        features = self.encoder.extract_features(x)         # (B, 256)\n        projections = self.projector(features)              # (B, 128)\n        return nn.functional.normalize(projections, dim=1)  # L2 normalization for contrastive loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:28.323724Z","iopub.execute_input":"2025-06-19T15:24:28.324024Z","iopub.status.idle":"2025-06-19T15:24:28.340724Z","shell.execute_reply.started":"2025-06-19T15:24:28.324003Z","shell.execute_reply":"2025-06-19T15:24:28.339631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- NT-Xent Loss (Contrastive) ---\nclass NTXentLoss(nn.Module):\n    def __init__(self, batch_size, temperature=0.5):\n        \"\"\"\n        Implements the Normalized Temperature-scaled Cross Entropy loss\n        as used in SimCLR.\n        \"\"\"\n        super(NTXentLoss, self).__init__()\n        self.batch_size = batch_size\n        self.temperature = temperature\n        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n        self.mask = self._get_correlated_mask().type(torch.bool)\n\n    def _get_correlated_mask(self):\n        N = 2 * self.batch_size\n        mask = torch.ones((N, N)) - torch.eye(N)\n        return mask\n\n    def forward(self, zis, zjs):\n        \"\"\"\n        zis and zjs: (B, D) projections from two augmented views\n        \"\"\"\n        device = zis.device\n        N = 2 * self.batch_size\n\n        # Concatenate all projections\n        z = torch.cat([zis, zjs], dim=0)  # (2N, D)\n        \n        # Cosine similarity between all pairs\n        sim_matrix = nn.functional.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)  # (2N, 2N)\n\n        # Positive pairs are at offsets ±batch_size\n        positives = torch.cat([\n            torch.diag(sim_matrix, self.batch_size),\n            torch.diag(sim_matrix, -self.batch_size)\n        ], dim=0)\n\n        # All other pairs are negatives\n        negatives = sim_matrix[self.mask.to(device)].view(N, -1)\n\n        # Construct logits and labels\n        logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n        labels = torch.zeros(N).long().to(device)  # positive pair = class 0\n\n        # Scale by temperature and apply loss\n        logits = logits / self.temperature\n        return self.criterion(logits, labels) / N\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:28.341729Z","iopub.execute_input":"2025-06-19T15:24:28.342002Z","iopub.status.idle":"2025-06-19T15:24:28.359488Z","shell.execute_reply.started":"2025-06-19T15:24:28.341975Z","shell.execute_reply":"2025-06-19T15:24:28.358370Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SSL Training - Setup\n","metadata":{}},{"cell_type":"code","source":"# --- SSL Dataset Class for SimCLR ---\nclass SSLFoodDataset(Dataset):\n    \"\"\"\n    Dataset for SimCLR-style self-supervised training.\n    Returns two differently augmented views of the same image.\n    \"\"\"\n    def __init__(self, image_dir, transform, image_names=None):\n        self.image_dir = image_dir\n        self.transform = transform\n        self.image_names = image_names or os.listdir(image_dir)\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        img_name = self.image_names[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n        \n        xi = self.transform(image)\n        xj = self.transform(image)\n        return xi, xj\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:28.361123Z","iopub.execute_input":"2025-06-19T15:24:28.361512Z","iopub.status.idle":"2025-06-19T15:24:28.382709Z","shell.execute_reply.started":"2025-06-19T15:24:28.361477Z","shell.execute_reply":"2025-06-19T15:24:28.381624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- SSL Training Parameters ---\nbatch_size_ssl = 48 # Works best with kaggle GPU \n\nssl_dataset = SSLFoodDataset(\n    image_dir=working_dir / 'train',\n    transform=simclr_transform\n)\n\nssl_loader = DataLoader(\n    ssl_dataset,\n    batch_size=batch_size_ssl,\n    shuffle=True,\n    num_workers=2,  # Adjust if needed\n    pin_memory=True,\n    persistent_workers=True,\n    drop_last=True,  # Required for SimCLR (2N logic)\n    worker_init_fn=seed_worker\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:28.383782Z","iopub.execute_input":"2025-06-19T15:24:28.384134Z","iopub.status.idle":"2025-06-19T15:24:28.463315Z","shell.execute_reply.started":"2025-06-19T15:24:28.384101Z","shell.execute_reply":"2025-06-19T15:24:28.462320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Model, Optimizer, Loss ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nbase_encoder = build_custom_cnn(num_classes=251)  # num_classes irrelevant for SSL\nmodel = SimCLR(base_encoder=base_encoder).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = NTXentLoss(batch_size=batch_size_ssl, temperature=0.5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:24:28.464171Z","iopub.execute_input":"2025-06-19T15:24:28.464460Z","iopub.status.idle":"2025-06-19T15:24:28.549302Z","shell.execute_reply.started":"2025-06-19T15:24:28.464438Z","shell.execute_reply":"2025-06-19T15:24:28.548252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SSL Pretraining Loop","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\n\n\n\n# Number epochs for SSL Pretraining\nssl_epochs = 20 # Adjust\nmodel.train()\n\nfor epoch in range(ssl_epochs):\n    total_loss = 0.0\n    num_batches = 0\n\n    progress_bar = tqdm(ssl_loader, desc=f\"Epoch {epoch+1}/{ssl_epochs}\")\n    \n    for xi, xj in progress_bar:\n        xi, xj = xi.to(device), xj.to(device)\n\n        # Embeddings \n        zis = model(xi)  # (B, D)\n        zjs = model(xj)  # (B, D)\n\n        # NT-Xent Loss\n        loss = loss_fn(zis, zjs)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        num_batches += 1\n        progress_bar.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / num_batches\n    print(f\"[Epoch {epoch+1}] Average Loss: {avg_loss:.4f}\")\n    if (epoch + 1) % 4 == 0:  # Alle 4 Epochen \n        torch.save(model.state_dict(), f\"/kaggle/working/simclr_checkpoint_ep{epoch+1}.pth\")\n        print(torch.cuda.memory_summary(device=None, abbreviated=True))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T18:09:55.388366Z","iopub.execute_input":"2025-06-18T18:09:55.388533Z","execution_failed":"2025-06-18T18:15:05.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\ntorch.save(model.state_dict(), \"/kaggle/working/simclr_final.pth\")\n# shutil.copy(\"/kaggle/working/simclr_final.pth\", \"/kaggle/outputs/\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-18T18:15:05.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Extracting Feature Vectors\n","metadata":{}},{"cell_type":"code","source":"# --- Feature Extraction from Frozen SSL Encoder ---\n# Load pretrained encoder weights\nmodel.load_state_dict(torch.load(\"/kaggle/input/ssl_final/pytorch/default/1/simclr_final.pth\", map_location=device))\nmodel = model.to(device).eval()\n\n# Freeze encoder weights (no gradient updates)\nfor param in model.parameters():\n    param.requires_grad = False\n\n\n'''\nWe just have to execute this once with our best model and save the feature matrices. \nWe can load those matrices again for training the traditional classifiers.\n'''\n\n# --- Extract Features & Labels from Dataloader ---\ndef extract_features_and_labels(dataloader, model, device):\n    \"\"\"\n    Extracts feature vectors using the frozen encoder\n    and returns numpy arrays for downstream classifier training.\n    \"\"\"\n    features, labels = [], []\n    with torch.no_grad():\n        for images, lbls in tqdm(dataloader, desc=\"Extracting features\"):\n            images = images.to(device)\n            feats = model.encoder.extract_features(images).cpu().numpy()\n            features.append(feats)\n            labels.append(lbls.numpy())\n    return np.concatenate(features), np.concatenate(labels)\n\n# --- Run once to cache feature matrices ---\nX_train, y_train = extract_features_and_labels(train_loader, model, device)\nX_val, y_val     = extract_features_and_labels(val_loader, model, device)\nX_test, y_test   = extract_features_and_labels(test_loader, model, device)\n\n# Save to disk for reuse (e.g. in classifier experiments)\nnp.save(\"X_train.npy\", X_train)\nnp.save(\"y_train.npy\", y_train)\nnp.save(\"X_val.npy\", X_val)\nnp.save(\"y_val.npy\", y_val)\nnp.save(\"X_test.npy\", X_test)\nnp.save(\"y_test.npy\", y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:12:55.437860Z","iopub.execute_input":"2025-06-19T15:12:55.438102Z","iopub.status.idle":"2025-06-19T15:14:13.618305Z","shell.execute_reply.started":"2025-06-19T15:12:55.438085Z","shell.execute_reply":"2025-06-19T15:14:13.617268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training a traditional classifyer on feature vectors\n\nFollowing the project instructions, we extracted features from a CNN encoder trained with SimCLR,\nthen trained a set of traditional classifiers on top of those features:\n\n1. Logistic Regression\n2. Linear SVM (via SGDClassifier)\n3. Linear SVM trained only on validation set (limited label scenario)\n4. RBF SVM (trained on a 3k validation subset due to runtime)\n5. Random Forest\n6. MLP (trained separately in PyTorch)\n\nThis allows us to evaluate the quality of the learned representations without relying on end-to-end backpropagation.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# --- Load feature vectors ---\nX_train = np.load('/kaggle/input/features/X_train.npy')\ny_train = np.load('/kaggle/input/features/y_train.npy')\n\nX_val = np.load('/kaggle/input/features2/X_val.npy')\ny_val = np.load('/kaggle/input/features2/y_val.npy')\n\nX_test = np.load('/kaggle/input/features2/X_test.npy')\ny_test = np.load('/kaggle/input/features2/y_test.npy')\n\n# --- Evaluation helper ---\n\ndef evaluate_model(y_true, y_pred):\n    return {\n        'Accuracy_test': accuracy_score(y_true, y_pred),\n        'F1_weighted': f1_score(y_true, y_pred, average='weighted'),\n        'F1_macro': f1_score(y_true, y_pred, average='macro'),\n        'Precision_macro': precision_score(y_true, y_pred, average='macro'),\n        'Recall_macro': recall_score(y_true, y_pred, average='macro'),\n    }\n\n# --- Model training and evaluation ---\nresults = {}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T23:47:27.241987Z","iopub.execute_input":"2025-06-19T23:47:27.242415Z","iopub.status.idle":"2025-06-19T23:47:32.323628Z","shell.execute_reply.started":"2025-06-19T23:47:27.242383Z","shell.execute_reply":"2025-06-19T23:47:32.322461Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# 1. Logistic Regression (Full train set)\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\nresults['LogReg (full)'] = evaluate_model(y_test, lr.predict(X_test))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2.SGDClassifier (Full train set) as faster alternative to linear SVM\nfrom sklearn.linear_model import SGDClassifier\n\nsvm_sgd = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3)\nsvm_sgd.fit(X_train, y_train)\nresults['Linear SVM (full)'] = evaluate_model(y_test, svm_sgd.predict(X_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T23:49:43.966151Z","iopub.execute_input":"2025-06-19T23:49:43.966461Z","iopub.status.idle":"2025-06-19T23:51:37.054163Z","shell.execute_reply.started":"2025-06-19T23:49:43.966440Z","shell.execute_reply":"2025-06-19T23:51:37.053362Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 3. Random Forest (Full train set)\nrf_full = RandomForestClassifier(n_estimators=100, max_depth=20, n_jobs=-1)\nrf_full.fit(X_train, y_train)\nresults['Random Forest (full)'] = evaluate_model(y_test, rf_full.predict(X_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T23:16:16.295265Z","iopub.execute_input":"2025-06-19T23:16:16.295610Z","iopub.status.idle":"2025-06-19T23:21:22.476925Z","shell.execute_reply.started":"2025-06-19T23:16:16.295578Z","shell.execute_reply":"2025-06-19T23:21:22.475800Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 4. Linear SVM  (Val set only)\nsvm_linear_val = LinearSVC()\nsvm_linear_val.fit(X_val, y_val)\nresults['Linear SVM (val only)'] = evaluate_model(y_test, svm_linear_val.predict(X_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T00:26:12.860151Z","iopub.execute_input":"2025-06-20T00:26:12.860495Z","iopub.status.idle":"2025-06-20T00:42:50.260029Z","shell.execute_reply.started":"2025-06-20T00:26:12.860471Z","shell.execute_reply":"2025-06-20T00:42:50.258759Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1438349535.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msvm_linear_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msvm_linear_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Linear SVM (val only)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'svm_linear' is not defined"],"ename":"NameError","evalue":"name 'svm_linear' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# 5. RBF SVM (Subset of val set because its so slow otherwise)\nX_val_sub, _, y_val_sub, _ = train_test_split(X_val, y_val, train_size=3000, stratify=y_val)\nsvm_rbf = SVC(kernel='rbf')\nsvm_rbf.fit(X_val_sub, y_val_sub)\nresults['RBF SVM (val subset)'] = evaluate_model(y_test, svm_rbf.predict(X_test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T01:24:18.820427Z","iopub.execute_input":"2025-06-20T01:24:18.821164Z","iopub.status.idle":"2025-06-20T01:24:47.769101Z","shell.execute_reply.started":"2025-06-20T01:24:18.821128Z","shell.execute_reply":"2025-06-20T01:24:47.768180Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# --- Display as DataFrame ---\ndf_results = pd.DataFrame(results).T  # transpose for better readability\nprint(df_results)\n\ndf_results['Accuracy'] = pd.to_numeric(df_results['Accuracy'], errors='coerce')\n\n\ndf_results = df_results.sort_values(by='Accuracy', ascending=False)\nprint(df_results)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T23:06:47.891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Load extracted feature vectors from disk if not done already ---\nX_train = np.load('/kaggle/input/features/X_train.npy')\ny_train = np.load('/kaggle/input/features/y_train.npy')\n\nX_val = np.load('/kaggle/input/features/X_val.npy')\ny_val = np.load('/kaggle/input/features/y_val.npy')\n\nX_test = np.load('/kaggle/input/features/X_test.npy')\ny_test = np.load('/kaggle/input/features/y_test.npy')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Convert to PyTorch tensors ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor   = torch.tensor(y_val, dtype=torch.long)\nX_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n\n# --- Create DataLoaders ---\nmlp_train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=256, shuffle=True)\nmlp_val_loader   = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=256)\nmlp_test_loader  = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=256)\n\n\n# --- Define simple 2-layer MLP ---\nclass MLP(nn.Module):\n    def __init__(self, input_dim=256, hidden_dim=512, output_dim=251):\n        super(MLP, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# --- MLP Training Setup ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmlp_model = MLP(input_dim=X_train.shape[1]).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-3)\n\n# Early stopping\nbest_val_acc = 0\npatience = 5\nepochs_no_improve = 0\nn_epochs = 50 # 50 is okay since we have early stopping\n\nfor epoch in range(n_epochs):\n    # --- Training ---\n    mlp_model.train()\n    train_loss, correct, total = 0, 0, 0\n    for x_batch, y_batch in mlp_train_loader:\n        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        out = mlp_model(x_batch)\n        loss = criterion(out, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * x_batch.size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == y_batch).sum().item()\n        total += y_batch.size(0)\n\n    train_acc = correct / total\n    avg_train_loss = train_loss / total\n\n    # --- Validation ---\n    mlp_model.eval()\n    val_loss, val_correct, val_total = 0, 0, 0\n    with torch.no_grad():\n        for x_batch, y_batch in mlp_val_loader:\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n            out = mlp_model(x_batch)\n            loss = criterion(out, y_batch)\n\n            val_loss += loss.item() * x_batch.size(0)\n            preds = out.argmax(dim=1)\n            val_correct += (preds == y_batch).sum().item()\n            val_total += y_batch.size(0)\n\n    val_acc = val_correct / val_total\n    avg_val_loss = val_loss / val_total\n\n    print(f\"Epoch {epoch+1}: \"\n          f\"Train Loss = {avg_train_loss:.4f}, Acc = {train_acc:.4f} | \"\n          f\"Val Loss = {avg_val_loss:.4f}, Acc = {val_acc:.4f}\")\n\n    # --- Early Stopping ---\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_model_state = mlp_model.state_dict()\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(f\"Early stopping triggered at epoch {epoch+1}\")\n            break\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T23:08:43.061577Z","iopub.execute_input":"2025-06-19T23:08:43.062117Z","iopub.status.idle":"2025-06-19T23:09:49.164505Z","shell.execute_reply.started":"2025-06-19T23:08:43.062091Z","shell.execute_reply":"2025-06-19T23:09:49.163880Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Train Loss = 4.8205, Acc = 0.0626 | Val Loss = 4.4947, Acc = 0.0896\nEpoch 2: Train Loss = 4.3761, Acc = 0.1079 | Val Loss = 4.3352, Acc = 0.1097\nEpoch 3: Train Loss = 4.2426, Acc = 0.1247 | Val Loss = 4.2651, Acc = 0.1179\nEpoch 4: Train Loss = 4.1620, Acc = 0.1342 | Val Loss = 4.2010, Acc = 0.1297\nEpoch 5: Train Loss = 4.1031, Acc = 0.1430 | Val Loss = 4.1628, Acc = 0.1372\nEpoch 6: Train Loss = 4.0577, Acc = 0.1510 | Val Loss = 4.1462, Acc = 0.1375\nEpoch 7: Train Loss = 4.0132, Acc = 0.1558 | Val Loss = 4.1147, Acc = 0.1439\nEpoch 8: Train Loss = 3.9787, Acc = 0.1616 | Val Loss = 4.0875, Acc = 0.1500\nEpoch 9: Train Loss = 3.9450, Acc = 0.1666 | Val Loss = 4.0692, Acc = 0.1544\nEpoch 10: Train Loss = 3.9181, Acc = 0.1715 | Val Loss = 4.0609, Acc = 0.1549\nEpoch 11: Train Loss = 3.8917, Acc = 0.1757 | Val Loss = 4.0582, Acc = 0.1564\nEpoch 12: Train Loss = 3.8652, Acc = 0.1784 | Val Loss = 4.0444, Acc = 0.1595\nEpoch 13: Train Loss = 3.8442, Acc = 0.1821 | Val Loss = 4.0375, Acc = 0.1614\nEpoch 14: Train Loss = 3.8211, Acc = 0.1851 | Val Loss = 4.0254, Acc = 0.1629\nEpoch 15: Train Loss = 3.8032, Acc = 0.1879 | Val Loss = 4.0181, Acc = 0.1647\nEpoch 16: Train Loss = 3.7835, Acc = 0.1910 | Val Loss = 4.0131, Acc = 0.1686\nEpoch 17: Train Loss = 3.7644, Acc = 0.1947 | Val Loss = 4.0104, Acc = 0.1667\nEpoch 18: Train Loss = 3.7485, Acc = 0.1961 | Val Loss = 4.0100, Acc = 0.1684\nEpoch 19: Train Loss = 3.7337, Acc = 0.1977 | Val Loss = 4.0023, Acc = 0.1716\nEpoch 20: Train Loss = 3.7160, Acc = 0.2028 | Val Loss = 4.0090, Acc = 0.1716\nEpoch 21: Train Loss = 3.6994, Acc = 0.2039 | Val Loss = 4.0032, Acc = 0.1711\nEpoch 22: Train Loss = 3.6853, Acc = 0.2066 | Val Loss = 4.0037, Acc = 0.1721\nEpoch 23: Train Loss = 3.6742, Acc = 0.2081 | Val Loss = 3.9952, Acc = 0.1737\nEpoch 24: Train Loss = 3.6579, Acc = 0.2108 | Val Loss = 3.9896, Acc = 0.1749\nEpoch 25: Train Loss = 3.6474, Acc = 0.2108 | Val Loss = 3.9909, Acc = 0.1746\nEpoch 26: Train Loss = 3.6337, Acc = 0.2134 | Val Loss = 3.9890, Acc = 0.1758\nEpoch 27: Train Loss = 3.6240, Acc = 0.2153 | Val Loss = 3.9865, Acc = 0.1778\nEpoch 28: Train Loss = 3.6068, Acc = 0.2175 | Val Loss = 3.9969, Acc = 0.1766\nEpoch 29: Train Loss = 3.5949, Acc = 0.2206 | Val Loss = 3.9968, Acc = 0.1778\nEpoch 30: Train Loss = 3.5857, Acc = 0.2214 | Val Loss = 3.9974, Acc = 0.1758\nEpoch 31: Train Loss = 3.5737, Acc = 0.2232 | Val Loss = 3.9956, Acc = 0.1789\nEpoch 32: Train Loss = 3.5672, Acc = 0.2231 | Val Loss = 3.9986, Acc = 0.1777\nEpoch 33: Train Loss = 3.5519, Acc = 0.2265 | Val Loss = 3.9901, Acc = 0.1804\nEpoch 34: Train Loss = 3.5474, Acc = 0.2263 | Val Loss = 3.9913, Acc = 0.1795\nEpoch 35: Train Loss = 3.5363, Acc = 0.2274 | Val Loss = 3.9951, Acc = 0.1787\nEpoch 36: Train Loss = 3.5275, Acc = 0.2309 | Val Loss = 4.0006, Acc = 0.1803\nEpoch 37: Train Loss = 3.5177, Acc = 0.2312 | Val Loss = 3.9985, Acc = 0.1806\nEpoch 38: Train Loss = 3.5093, Acc = 0.2330 | Val Loss = 4.0065, Acc = 0.1805\nEpoch 39: Train Loss = 3.4983, Acc = 0.2346 | Val Loss = 4.0087, Acc = 0.1804\nEpoch 40: Train Loss = 3.4907, Acc = 0.2357 | Val Loss = 4.0002, Acc = 0.1808\nEpoch 41: Train Loss = 3.4838, Acc = 0.2352 | Val Loss = 4.0193, Acc = 0.1800\nEpoch 42: Train Loss = 3.4718, Acc = 0.2388 | Val Loss = 4.0109, Acc = 0.1817\nEpoch 43: Train Loss = 3.4656, Acc = 0.2389 | Val Loss = 4.0131, Acc = 0.1821\nEpoch 44: Train Loss = 3.4608, Acc = 0.2389 | Val Loss = 4.0191, Acc = 0.1808\nEpoch 45: Train Loss = 3.4499, Acc = 0.2423 | Val Loss = 4.0144, Acc = 0.1821\nEpoch 46: Train Loss = 3.4391, Acc = 0.2432 | Val Loss = 4.0190, Acc = 0.1818\nEpoch 47: Train Loss = 3.4343, Acc = 0.2433 | Val Loss = 4.0302, Acc = 0.1815\nEpoch 48: Train Loss = 3.4306, Acc = 0.2455 | Val Loss = 4.0235, Acc = 0.1811\nEarly stopping triggered at epoch 48\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# --- Load best model and test ---\nmlp_model.load_state_dict(best_model_state)\n\nfrom sklearn.metrics import classification_report\n\nmlp_model.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for x_batch, y_batch in mlp_test_loader:\n        x_batch = x_batch.to(device)\n        preds = mlp_model(x_batch).argmax(dim=1).cpu()\n        all_preds.extend(preds.numpy())\n        all_labels.extend(y_batch.numpy())\n\n# Final evaluation\nprint(classification_report(all_labels, all_preds, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T23:10:19.789041Z","iopub.execute_input":"2025-06-19T23:10:19.789347Z","iopub.status.idle":"2025-06-19T23:10:19.917496Z","shell.execute_reply.started":"2025-06-19T23:10:19.789325Z","shell.execute_reply":"2025-06-19T23:10:19.916677Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.2419    0.5455    0.3352        55\n           1     0.4865    0.2951    0.3673        61\n           2     0.5556    0.0943    0.1613        53\n           3     0.1485    0.2941    0.1974        51\n           4     0.4211    0.1951    0.2667        41\n           5     0.2245    0.2115    0.2178        52\n           6     0.2609    0.1053    0.1500        57\n           7     0.0465    0.0370    0.0412        54\n           8     0.2262    0.3878    0.2857        49\n           9     0.2353    0.1739    0.2000        46\n          10     0.1667    0.0208    0.0370        48\n          11     0.2024    0.3617    0.2595        47\n          12     0.1818    0.1778    0.1798        45\n          13     0.1875    0.0984    0.1290        61\n          14     0.0588    0.0217    0.0317        46\n          15     0.3333    0.2807    0.3048        57\n          16     0.3077    0.4898    0.3780        49\n          17     0.0952    0.0351    0.0513        57\n          18     0.0000    0.0000    0.0000        31\n          19     0.2105    0.0816    0.1176        49\n          20     0.0732    0.0714    0.0723        42\n          21     0.4857    0.3208    0.3864        53\n          22     0.0923    0.1429    0.1121        42\n          23     0.2468    0.3393    0.2857        56\n          24     0.1765    0.0556    0.0845        54\n          25     0.1910    0.2931    0.2313        58\n          26     0.3382    0.4035    0.3680        57\n          27     0.2603    0.3519    0.2992        54\n          28     0.2444    0.2037    0.2222        54\n          29     0.1186    0.1707    0.1400        41\n          30     0.2444    0.2115    0.2268        52\n          31     0.1324    0.1607    0.1452        56\n          32     0.1053    0.0816    0.0920        49\n          33     0.1250    0.0244    0.0408        41\n          34     0.4857    0.2537    0.3333        67\n          35     0.1848    0.2742    0.2208        62\n          36     0.1622    0.1071    0.1290        56\n          37     0.1250    0.0192    0.0333        52\n          38     0.2442    0.3750    0.2958        56\n          39     0.0769    0.0189    0.0303        53\n          40     0.0000    0.0000    0.0000        35\n          41     0.1690    0.4000    0.2376        60\n          42     0.1975    0.2963    0.2370        54\n          43     0.3488    0.2381    0.2830        63\n          44     0.1481    0.0976    0.1176        41\n          45     0.2625    0.4565    0.3333        46\n          46     0.2453    0.4727    0.3230        55\n          47     0.0575    0.1163    0.0769        43\n          48     0.2969    0.3393    0.3167        56\n          49     0.5833    0.4828    0.5283        58\n          50     0.2667    0.1250    0.1702        32\n          51     0.4182    0.4035    0.4107        57\n          52     0.4828    0.2692    0.3457        52\n          53     0.2712    0.2963    0.2832        54\n          54     0.1489    0.1892    0.1667        37\n          55     0.1176    0.0800    0.0952        50\n          56     0.1266    0.1754    0.1471        57\n          57     0.1692    0.2157    0.1897        51\n          58     0.1429    0.0227    0.0392        44\n          59     0.1235    0.2857    0.1724        35\n          60     0.1571    0.4151    0.2280        53\n          61     0.1967    0.2609    0.2243        46\n          62     0.2500    0.4651    0.3252        43\n          63     0.6061    0.4082    0.4878        49\n          64     0.2118    0.3462    0.2628        52\n          65     0.0000    0.0000    0.0000        24\n          66     0.2833    0.3036    0.2931        56\n          67     0.3636    0.1538    0.2162        52\n          68     0.1375    0.3667    0.2000        60\n          69     0.3478    0.1633    0.2222        49\n          70     0.4848    0.2857    0.3596        56\n          71     0.2439    0.2778    0.2597        36\n          72     0.1739    0.4103    0.2443        39\n          73     0.2500    0.0877    0.1299        57\n          74     0.0800    0.0400    0.0533        50\n          75     0.2035    0.3770    0.2644        61\n          76     0.2034    0.2264    0.2143        53\n          77     0.1392    0.1930    0.1618        57\n          78     0.2459    0.4167    0.3093        36\n          79     0.2500    0.4000    0.3077        50\n          80     0.3333    0.0851    0.1356        47\n          81     0.2787    0.2982    0.2881        57\n          82     0.1887    0.3226    0.2381        62\n          83     0.5000    0.0769    0.1333        26\n          84     0.2500    0.1500    0.1875        40\n          85     0.0000    0.0000    0.0000        38\n          86     0.5769    0.3191    0.4110        47\n          87     0.2889    0.2281    0.2549        57\n          88     0.4545    0.2586    0.3297        58\n          89     0.4545    0.2273    0.3030        44\n          90     0.1456    0.2632    0.1875        57\n          91     0.0769    0.0465    0.0580        43\n          92     0.0606    0.0488    0.0541        41\n          93     0.2381    0.1163    0.1562        43\n          94     0.1509    0.1633    0.1569        49\n          95     0.3878    0.3800    0.3838        50\n          96     0.1818    0.1176    0.1429        51\n          97     0.4118    0.3443    0.3750        61\n          98     0.0968    0.0517    0.0674        58\n          99     0.2727    0.1429    0.1875        21\n         100     0.4182    0.4894    0.4510        47\n         101     0.1667    0.2222    0.1905        36\n         102     0.2364    0.2826    0.2574        46\n         103     0.0714    0.0714    0.0714        42\n         104     0.0571    0.1500    0.0828        40\n         105     0.5455    0.1176    0.1935        51\n         106     0.1852    0.2222    0.2020        45\n         107     0.3571    0.2083    0.2632        48\n         108     0.5000    0.1111    0.1818        36\n         109     0.2188    0.2000    0.2090        35\n         110     0.3256    0.2545    0.2857        55\n         111     0.3571    0.5000    0.4167        60\n         112     0.1250    0.2037    0.1549        54\n         113     0.1447    0.4423    0.2180        52\n         114     0.1795    0.1321    0.1522        53\n         115     0.0952    0.0952    0.0952        42\n         116     0.0000    0.0000    0.0000         2\n         117     0.0769    0.0741    0.0755        27\n         118     0.0000    0.0000    0.0000        43\n         119     0.1552    0.1406    0.1475        64\n         120     0.0000    0.0000    0.0000        36\n         121     0.4000    0.1667    0.2353        48\n         122     0.0439    0.0862    0.0581        58\n         123     0.2353    0.0930    0.1333        43\n         124     0.2000    0.5227    0.2893        44\n         125     0.0968    0.0833    0.0896        36\n         126     0.1616    0.3902    0.2286        41\n         127     0.0833    0.0263    0.0400        38\n         128     0.2500    0.1481    0.1860        54\n         129     0.2500    0.0857    0.1277        35\n         130     0.0385    0.0233    0.0290        43\n         131     0.3333    0.3962    0.3621        53\n         132     0.0000    0.0000    0.0000        55\n         133     0.4500    0.1406    0.2143        64\n         134     0.3333    0.1481    0.2051        54\n         135     0.2658    0.3962    0.3182        53\n         136     0.4074    0.2245    0.2895        49\n         137     0.1724    0.1042    0.1299        48\n         138     0.2119    0.4237    0.2825        59\n         139     0.1892    0.1458    0.1647        48\n         140     0.0759    0.1500    0.1008        40\n         141     0.1150    0.2600    0.1595        50\n         142     0.2676    0.3220    0.2923        59\n         143     0.2414    0.4000    0.3011        35\n         144     0.3165    0.5435    0.4000        46\n         145     0.1429    0.1395    0.1412        43\n         146     0.1190    0.1190    0.1190        42\n         147     0.1091    0.1200    0.1143        50\n         148     0.3182    0.1129    0.1667        62\n         149     0.0000    0.0000    0.0000        22\n         150     0.1667    0.2321    0.1940        56\n         151     0.1333    0.1455    0.1391        55\n         152     0.0610    0.1111    0.0787        45\n         153     0.3784    0.2456    0.2979        57\n         154     0.2222    0.1000    0.1379        60\n         155     0.2326    0.1961    0.2128        51\n         156     0.0930    0.1111    0.1013        36\n         157     0.1020    0.0962    0.0990        52\n         158     0.1905    0.1538    0.1702        52\n         159     0.2143    0.1154    0.1500        26\n         160     0.1905    0.1250    0.1509        64\n         161     0.4400    0.4490    0.4444        49\n         162     0.0000    0.0000    0.0000        49\n         163     0.0723    0.1818    0.1034        33\n         164     0.4444    0.3243    0.3750        37\n         165     0.2381    0.1389    0.1754        36\n         166     0.1522    0.1429    0.1474        49\n         167     0.1875    0.0508    0.0800        59\n         168     0.2593    0.5385    0.3500        65\n         169     0.1091    0.2553    0.1529        47\n         170     0.0588    0.0286    0.0385        35\n         171     0.0339    0.0392    0.0364        51\n         172     0.1772    0.2500    0.2074        56\n         173     0.3457    0.5283    0.4179        53\n         174     0.2632    0.1163    0.1613        43\n         175     0.0000    0.0000    0.0000        37\n         176     0.0435    0.0238    0.0308        42\n         177     0.3553    0.4219    0.3857        64\n         178     0.1698    0.1475    0.1579        61\n         179     0.2000    0.0943    0.1282        53\n         180     0.1282    0.1190    0.1235        42\n         181     0.3056    0.2558    0.2785        43\n         182     0.0357    0.0175    0.0235        57\n         183     0.3333    0.0417    0.0741        48\n         184     0.3077    0.1379    0.1905        58\n         185     0.3000    0.1765    0.2222        51\n         186     0.5000    0.2157    0.3014        51\n         187     0.1452    0.2308    0.1782        39\n         188     0.2903    0.1636    0.2093        55\n         189     0.0000    0.0000    0.0000         9\n         190     0.0541    0.0392    0.0455        51\n         191     0.1739    0.3529    0.2330        34\n         192     0.2500    0.0217    0.0400        46\n         193     0.2500    0.0833    0.1250        48\n         194     0.2121    0.1591    0.1818        44\n         195     0.2609    0.2927    0.2759        41\n         196     0.0909    0.0545    0.0682        55\n         197     0.0656    0.2162    0.1006        37\n         198     0.2281    0.3250    0.2680        40\n         199     0.1000    0.0357    0.0526        56\n         200     0.2222    0.4000    0.2857        50\n         201     0.4301    0.5797    0.4938        69\n         202     0.1111    0.0286    0.0455        35\n         203     0.1449    0.3636    0.2073        55\n         204     0.3333    0.1579    0.2143        57\n         205     0.2632    0.1064    0.1515        47\n         206     0.1538    0.2553    0.1920        47\n         207     0.1707    0.1489    0.1591        47\n         208     0.0566    0.0638    0.0600        47\n         209     0.0000    0.0000    0.0000        41\n         210     0.1190    0.1000    0.1087        50\n         211     0.1000    0.1667    0.1250        42\n         212     0.0000    0.0000    0.0000        52\n         213     0.0000    0.0000    0.0000        23\n         214     0.3529    0.1429    0.2034        42\n         215     0.0517    0.0882    0.0652        34\n         216     0.1923    0.1087    0.1389        46\n         217     0.1573    0.2800    0.2014        50\n         218     0.0909    0.1000    0.0952        30\n         219     0.3455    0.3333    0.3393        57\n         220     0.5500    0.2075    0.3014        53\n         221     0.1471    0.1087    0.1250        46\n         222     0.1591    0.1489    0.1538        47\n         223     0.2708    0.3095    0.2889        42\n         224     0.3000    0.1034    0.1538        58\n         225     0.1887    0.3774    0.2516        53\n         226     0.6875    0.2037    0.3143        54\n         227     0.4286    0.5094    0.4655        53\n         228     0.0000    0.0000    0.0000        42\n         229     0.1143    0.0702    0.0870        57\n         230     0.1481    0.2286    0.1798        35\n         231     0.2157    0.2558    0.2340        43\n         232     0.1649    0.2963    0.2119        54\n         233     0.5000    0.1379    0.2162        58\n         234     0.2381    0.1064    0.1471        47\n         235     0.2000    0.0909    0.1250        33\n         236     0.2326    0.5263    0.3226        57\n         237     0.0290    0.0500    0.0367        40\n         238     0.0000    0.0000    0.0000        11\n         239     0.1111    0.0588    0.0769        17\n         240     0.1143    0.1176    0.1159        34\n         241     0.1912    0.2063    0.1985        63\n         242     0.2222    0.0870    0.1250        46\n         243     0.0000    0.0000    0.0000        36\n         244     0.1000    0.0233    0.0377        43\n         245     0.3519    0.3115    0.3304        61\n         246     0.2105    0.1481    0.1739        54\n         247     0.0000    0.0000    0.0000        43\n         248     0.4516    0.2373    0.3111        59\n         249     0.3548    0.2340    0.2821        47\n         250     0.3788    0.5000    0.4310        50\n\n    accuracy                         0.2066     11994\n   macro avg     0.2128    0.1954    0.1852     11994\nweighted avg     0.2240    0.2066    0.1958     11994\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}