{"cells":[{"cell_type":"markdown","metadata":{"id":"9JG5D3B4A8nn"},"source":["\"\"\"\n","This script performs multiclass image classification on the iFood-2019 dataset using a custom CNN with residual blocks and CBAM attention.\n","\n","Main components:\n","1. Reproducibility setup with fixed seeds\n","2. Dataset extraction, transformation, and preparation\n","3. Custom dataset class and data loaders\n","4. Definition of a residual CNN with CBAM attention\n","5. Training with label smoothing, cosine LR scheduling, and early stopping\n","6. Hyperparameter grid search over learning rate and weight decay\n","7. Model evaluation with classification report and confusion matrix\n","8. Plots for accuracy, losses, errors, time over epoocs\n","\n","Output:\n","- Best model checkpoint\n","- Training and evaluation logs\n","- Accuracy and performance metrics\n","- Plots to suppor it\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"NaBPYrzQA8nq"},"outputs":[],"source":["#pip install tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"iL0NioXHA8nr"},"outputs":[],"source":["# Import libraries\n","import zipfile\n","import os\n","import shutil\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","import pandas as pd\n","from torchvision import datasets\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Subset\n","import numpy as np\n","import time\n","import os, pandas as pd, zipfile, pathlib\n","import torch, random, numpy as np\n","from pathlib import Path\n","import random\n","from tqdm import tqdm\n","import multiprocessing\n","from torch.cuda.amp import autocast, GradScaler\n","import torch\n","from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"9bLXOeqvA8ns"},"outputs":[],"source":["# fixed the seed - it is necessary for reproducibility of the results.\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# We use multiple workers. Each worker needs its own seed. If we dont fix the seed of the workers this might destroy the purpose of shuflle of augmnetations since each worker would generate it own sequence.\n","def seed_worker(worker_id):\n","    seed = torch.initial_seed() % 2**32\n","    np.random.seed(seed)\n","    random.seed(seed)\n","set_seed(42)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFwulIZPA8ns"},"outputs":[],"source":["# Setting the paths\n","data_dir = Path('/kaggle/input/ifood-2019-fgvc6')\n","working_dir = Path('/kaggle/working')\n","TRAIN_CSV = data_dir / 'train_labels.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"Ko-WPnTGA8ns"},"outputs":[],"source":["# Since the datset is zipped and contains an extra internal folder we do flatening. This was necessary to all pytorch to read the actual Imagefolder\n","\n","def extract_and_flatten(zip_filename, folder_name, internal_folder_name):\n","    zip_path = data_dir / zip_filename\n","    extract_path = working_dir / (folder_name + \"_temp\")\n","    final_path = working_dir / folder_name\n","\n","    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n","        zip_ref.extractall(extract_path)\n","\n","    nested = extract_path / internal_folder_name\n","    final_path.mkdir(exist_ok=True)\n","    for fname in os.listdir(nested):\n","        shutil.move(str(nested / fname), str(final_path / fname))\n","\n","    shutil.rmtree(extract_path)\n","\n","# We apply this to the three subests, train, validation split\n","extract_and_flatten(\"train_set.zip\", \"train\", \"train_set\")\n","extract_and_flatten(\"val_set.zip\", \"val\", \"val_set\")\n","extract_and_flatten(\"test_set.zip\", \"test\", \"test_set\")"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"Wi9Ob18QA8ns"},"outputs":[],"source":["# Ensure that we loaded the dataset correctly. Just a check\n","print(\"Train images:\", len(os.listdir(os.path.join(working_dir, \"train\"))))\n","print(\"Validation images:\", len(os.listdir(os.path.join(working_dir, \"val\"))))\n","print(\"Test images:\", len(os.listdir(os.path.join(working_dir, \"test\"))))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"bmC9QyNlA8nt"},"outputs":[],"source":["labels_df = pd.read_csv(labels_csv)\n"]},{"cell_type":"markdown","metadata":{"id":"g_FMtpTuA8nt"},"source":["In the cell below we descbe the tranfromation that we applied in the preprocessing part. First the mean and the standard deviation are not with the default Imagente Mean [0.485, 0.456, 0.406] Standard deviation [0.229, 0.224, 0.225] values. Instead we calculated the mean and the standar deviation of our traing set. The same values were used also for the normalisation of the validation set. The values that we obtained from the traing set were Mean [0.6388, 0.5444, 0.4448] and Std [0.2229, 0.2414, 0.2638]. And these are the ones used below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAvV4dhzA8nt"},"outputs":[],"source":["# def compute_mean_std(image_dir):\n","#     from tqdm import tqdm\n","\n","#     transform = transforms.Compose([\n","#         transforms.Resize((224, 224)),\n","#         transforms.ToTensor()\n","#     ])\n","\n","#     image_paths = list(Path(image_dir).glob(\"*.jpg\"))\n","\n","#     mean = torch.zeros(3)\n","#     std = torch.zeros(3)\n","#     total_images = 0\n","\n","#     for img_path in tqdm(image_paths, desc=\"Computing mean/std\"):\n","#         image = Image.open(img_path).convert(\"RGB\")\n","#         tensor = transform(image)\n","#         mean += tensor.mean(dim=(1, 2))\n","#         std += tensor.std(dim=(1, 2))\n","#         total_images += 1\n","\n","#     mean /= total_images\n","#     std /= total_images\n","\n","#     print(\"Mean:\", mean.tolist())\n","#     print(\"Std:\", std.tolist())\n","#     return mean.tolist(), std.tolist()"]},{"cell_type":"markdown","metadata":{"id":"uAQJHr1wA8nt"},"source":["Secondly, in our initial experiments, we observed that the training accuracy was significantly higher than the validation and test accuracy. To address this issue, we implemented augmentations such as random rotations, flips, and brightness changes. These were applied only to the training set, not to the test set.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"otA3SGs1A8nt"},"outputs":[],"source":["# Transforms ( with dataset-specific mean and std)\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    # prof of flipping 50  percent\n","    transforms.RandomHorizontalFlip(),\n","    # random slight rotations in range + or - 15 degres\n","    transforms.RandomRotation(15),\n","    # Randomly changes brightness, contrast, saturation, and hue\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n","    # slight random crops of the images\n","    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.6388, 0.5444, 0.4448], std=[0.2229, 0.2414, 0.2638])\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.6388, 0.5444, 0.4448], std=[0.2229, 0.2414, 0.2638])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"BDjsQ6RAA8nu"},"outputs":[],"source":["# Dataset\n","class FoodDataset(Dataset):\n","    def __init__(self, image_dir, labels_df, transform, class_to_idx):\n","        self.image_dir = image_dir\n","        self.labels_df = labels_df\n","        self.transform = transform\n","        #  Dictionary to map class names to numeric indices\n","        self.class_to_idx = class_to_idx\n","\n","    def __len__(self):\n","        return len(self.labels_df)\n","\n","    def __getitem__(self, idx):\n","        row = self.labels_df.iloc[idx]\n","        img_path = os.path.join(self.image_dir, row['img_name'])\n","        # convert to RGB\n","        image = Image.open(img_path).convert('RGB')\n","        label = self.class_to_idx[row['label']]\n","        image = self.transform(image)\n","        return image, label\n"]},{"cell_type":"markdown","metadata":{"id":"HO12Lb1fA8nu"},"source":["In the Kaggle dataset we used, the test images were not accompanied by ground truth labels. Therefore, we did not use the test set. Instead, we split the original training set into training and validation subsets (with stratification). As a result, we trained the model using only 80\\% of the original training set, while the remaining 20\\% was used for validation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2WmeMhzA8nu"},"outputs":[],"source":["# Load CSVs and split\n","labels_df = pd.read_csv(TRAIN_CSV)\n","classes = sorted(labels_df['label'].unique())\n","class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n","labels = labels_df['label'].map(class_to_idx)\n","\n","train_idx, val_idx = train_test_split(\n","    np.arange(len(labels_df)), test_size=0.2,\n","    stratify=labels, random_state=42\n",")\n","\n","train_df = labels_df.iloc[train_idx].reset_index(drop=True)\n","val_df = labels_df.iloc[val_idx].reset_index(drop=True)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZA0LR_YvA8nu"},"source":["A high number of workers can speed up the data loading process. However, using too many workers may lead to system overload. To avoid this, we defined a boundary by limiting the number of workers to a maximum value. The minimum was set to the number of available CPU cores. For reproducibility, we set the \\texttt{seed\\_worker}. Shuffling was applied only during training; for validation and testing, shuffling was disabled. Additionally, we enabled \\texttt{pin\\_memory=True} to allow direct memory access transfers to the GPU.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTV6RkSrA8nu"},"outputs":[],"source":["# Set optimal num_workers\n","cpu_count = multiprocessing.cpu_count()\n","num_workers = min(6, cpu_count)\n","\n","# Loaders\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,\n","                          num_workers=num_workers, pin_memory=True,\n","                          persistent_workers=True, worker_init_fn=seed_worker)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False,\n","                        num_workers=num_workers, pin_memory=True,\n","                        persistent_workers=True, worker_init_fn=seed_worker)\n","\n","test_csv = data_dir / 'val_labels.csv'\n","test_labels_df = pd.read_csv(test_csv)\n","test_dataset = FoodDataset(working_dir / 'val', test_labels_df, val_transform, class_to_idx)\n","test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False,\n","                         num_workers=num_workers, pin_memory=True,\n","                         persistent_workers=True, worker_init_fn=seed_worker)"]},{"cell_type":"markdown","metadata":{"id":"SzR3WF8zA8nu"},"source":["The network  ResNet-inspired convolutional neural network with under 5 million parameters. It uses stacked `BasicBlock` residual units to preserve gradient flow. The final feature map is refined with a CBAM attention module that applies both channel and spatial attention. A global average pooling layer, followed by dropout and a two-layer MLP, produces the final class logits for 251 food categories. The model employs `SiLU` activations and `AdamW` optimization with cosine learning rate scheduling.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWPNdgvLA8nu"},"outputs":[],"source":["# Basic residual block used in the CNN architecture\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.act = nn.SiLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.downsample = downsample  # Used when input and output dimensions don't match\n","\n","    def forward(self, x):\n","        identity = x if self.downsample is None else self.downsample(x)\n","        out = self.act(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += identity  # Residual connection\n","        return self.act(out)\n","\n","# Convolutional Block Attention Module. Aim: to improve feature focus\n","class CBAMBlock(nn.Module):\n","    def __init__(self, channels, reduction=16):\n","        super().__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","        self.channel_fc = nn.Sequential(\n","            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n","            nn.ReLU(),\n","            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n","        )\n","        self.spatial = nn.Sequential(\n","            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),\n","            nn.Sigmoid()\n","        )\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # Channel attention mechanism\n","        avg_out = self.channel_fc(self.avg_pool(x))\n","        max_out = self.channel_fc(self.max_pool(x))\n","        channel_att = self.sigmoid(avg_out + max_out)\n","        x = x * channel_att\n","\n","        # Spatial attention mechanism\n","        avg_pool = torch.mean(x, dim=1, keepdim=True)\n","        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n","        spatial_att = self.spatial(torch.cat([avg_pool, max_pool], dim=1))\n","        return x * spatial_att\n","\n","# Custom CNN architecture with residual blocks and CBAM attention\n","class CustomCNN(nn.Module):\n","    def __init__(self, block, layers, num_classes=251, base_width=32):\n","        super().__init__()\n","        self.in_channels = base_width\n","\n","        # Initial convolution + activation\n","        self.conv1 = nn.Conv2d(3, base_width, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(base_width)\n","        self.act = nn.SiLU(inplace=True)\n","\n","        # Residual layers\n","        self.layer1 = self._make_layer(block, base_width, layers[0])\n","        self.layer2 = self._make_layer(block, base_width * 2, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, base_width * 4, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, base_width * 8, layers[3], stride=2)\n","\n","        # Attention mechanism\n","        self.cbam = CBAMBlock(base_width * 8)\n","\n","        # Classification head\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout = nn.Dropout(0.3)\n","        self.fc = nn.Sequential(\n","            nn.Linear(base_width * 8, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","        self._init_weights()  # Initialize all weights\n","\n","    # residual layer with `blocks` number of blocks\n","    def _make_layer(self, block, out_channels, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.in_channels != out_channels * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels * block.expansion)\n","            )\n","        layers = [block(self.in_channels, out_channels, stride, downsample)]\n","        self.in_channels = out_channels * block.expansion\n","        layers.extend([block(self.in_channels, out_channels) for _ in range(1, blocks)])\n","        return nn.Sequential(*layers)\n","\n","    # Custom weight initialization\n","    def _init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","    # Forward pass through the entire netwrk\n","    def forward(self, x):\n","        x = self.act(self.bn1(self.conv1(x)))\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.cbam(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"dvbsi3uiA8nu"},"source":["The instantiation of the network goes as follows\n","     - BasicBlock as the residual block\n","     - [2, 2, 2, 2] blocks per layer (ResNet-18-like depth)\n","     - 40 base channels (slightly narrower than standard)\n","     - num_classes as output layer size (default: 251 for iFood).\n","Here we also ensured that our network is below the maximum threshold of 5M params."]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"vEiGe6KkA8nu","outputId":"cbab4737-5c33-428a-92fb-30abf3f12724"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainable parameters: 4,671,493 (4.67M)\n"]}],"source":["# Function to build and return the custom CNN model\n","def build_custom_cnn(num_classes=251):\n","\n","    model = CustomCNN(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, base_width=40)\n","\n","    # Count and print the number of trainable parameters\n","    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"Trainable parameters: {n_params:,} ({n_params / 1e6:.2f}M)\")\n","\n","    return model\n","\n","# Instantiate the model\n","model = build_custom_cnn(num_classes=251)\n"]},{"cell_type":"markdown","metadata":{"id":"tXVa1JeVA8nv"},"source":["Cross-entropy loss is used for multi-class classification. Since we dealt with a large dataset, it is important to reduce sharp probability peaks and discourage the model from being too confident. That is why, instead of assigning hard probabilities, we used label_smoothing=0.1 to assign 0.9 probability to the correct classes and 0.1 to the rest. This can help with overfitting."]},{"cell_type":"markdown","metadata":{"id":"VddnzewgA8nv"},"source":["AdamW =Adam with decoupled weight decay is the optimizer that we decided for. It is a variation of the Adam optimizer but uses L2 regularisation. The reeason we used this is overfitting."]},{"cell_type":"markdown","metadata":{"id":"HCWIlfk6A8nv"},"source":["To adjust the learning rate dynamically, we used CosineAnnealingLR. The idea is to start with a higher learning rate and decrease it over time to avoid overshooting a minimum point. We applied this scheduler to the AdamW optimizer. Note that T_max represents the number of epochs over which the learning rate decreases. Here, it is set to 50. If a different number of epochs is used, the T_max parameter should be adjusted accordingly."]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"_OGL1d0PA8nv","outputId":"ce0b4896-a0b5-4311-8117-ffca124b472d"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model = model.to(device)\n","\n","# Loss, optimizer, scheduler\n","criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"erVAteGhA8nv"},"source":["Since we conducted this project with limited computational resources, we aimed to avoid training models that failed to improve over time. To address this, we implemented early stopping. We set a patience value, which we varied between 6 and 10 depending on whether we were still in the experimental phase for a specific architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"ehqRrjpSA8nv"},"outputs":[],"source":["#  EarlyStopping class\n","class EarlyStopping:\n","    def __init__(self, patience=5, verbose=False, save_path='best_model10.pt'):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.save_path = save_path\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","\n","    def __call__(self, val_score, model):\n","        if self.best_score is None or val_score > self.best_score:\n","            self.best_score = val_score\n","            self.counter = 0\n","            if self.verbose:\n","                print(f\"Validation score improved. Saving model to {self.save_path}\")\n","            torch.save(model.state_dict(), self.save_path)\n","        else:\n","            self.counter += 1\n","            if self.verbose:\n","                print(f\"No improvement in validation score for {self.counter} epochs.\")\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n"]},{"cell_type":"markdown","metadata":{"id":"9658wp34A8nv"},"source":["The grid search is not extensive, since we got the impression that the actual difference between \"good performing\" neural networks actually shows after 20–25, maybe even after 30 epochs. This is the phase when validation accuracy increases above 30%, a point above which not every neural network can reach. Training neural networks with around 5 million parameters for that many epochs requires resources that we didn’t have."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5k8q226-A8nw"},"outputs":[],"source":["search_space = {\n","    'lr': [1e-4, 1e-3],\n","    'weight_decay': [0.01, 0.001],\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9FQYxV99A8nw"},"outputs":[],"source":["# Initialize tracking variables\n","best_val_acc = 0.0\n","best_config = None\n","best_model_state = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXW6aip_A8nw"},"outputs":[],"source":["# Set device and training parameters\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","num_epochs = 60\n","patience = 6  # for early stopping. If it doesnt improve after 6 epochs stop\n"]},{"cell_type":"markdown","metadata":{"id":"6JVyyYhjA8nw"},"source":["We performed a grid search over learning rate and weight decay combinations. For each configuration, the model was trained using AdamW with label smoothing, cosine annealing learning rate scheduling, and early stopping. Metrics and runtime were logged per epoch to identify the best-performing setup."]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"aLScy0NvA8nw"},"outputs":[],"source":["# Grid search over hyperparameter combinations\n","for lr in search_space['lr']:\n","    for wd in search_space['weight_decay']:\n","        print(f\"Training with lr={lr}, weight_decay={wd}\")\n","\n","        # Build model and training utilities\n","        model = build_custom_cnn(num_classes=251).to(device)\n","        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n","        early_stopper = EarlyStopping(patience=patience, verbose=False)\n","\n","        # Logs for analysis\n","        train_losses = []\n","        val_losses = []\n","        train_accuracies = []\n","        val_accuracies = []\n","        epoch_times = []\n","        total_start_time = time.time()\n","\n","        for epoch in range(num_epochs):\n","            epoch_start_time = time.time()\n","            model.train()\n","            train_loss, correct, total = 0.0, 0, 0\n","            loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n","\n","            # Training loop\n","            for images, labels in loop:\n","                images, labels = images.to(device), labels.to(device)\n","                optimizer.zero_grad()\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                train_loss += loss.item() * images.size(0)\n","                _, predicted = torch.max(outputs, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","                loop.set_postfix(loss=loss.item())\n","\n","            scheduler.step()  # update learning rate\n","\n","            # Compute average training metrics\n","            avg_train_loss = train_loss / total\n","            train_accuracy = 100 * correct / total\n","            train_losses.append(avg_train_loss)\n","            train_accuracies.append(train_accuracy)\n","\n","            # Validation loop\n","            model.eval()\n","            val_loss, val_correct, val_total = 0.0, 0, 0\n","            with torch.no_grad():\n","                for images, labels in val_loader:\n","                    images, labels = images.to(device), labels.to(device)\n","                    outputs = model(images)\n","                    loss = criterion(outputs, labels)\n","\n","                    val_loss += loss.item() * images.size(0)\n","                    _, predicted = torch.max(outputs, 1)\n","                    val_total += labels.size(0)\n","                    val_correct += (predicted == labels).sum().item()\n","\n","            # Compute average validation metrics\n","            avg_val_loss = val_loss / val_total\n","            val_accuracy = 100 * val_correct / val_total\n","            val_losses.append(avg_val_loss)\n","            val_accuracies.append(val_accuracy)\n","\n","            epoch_duration = time.time() - epoch_start_time\n","            epoch_times.append(epoch_duration)\n","\n","            print(f\"Epoch {epoch+1}/{num_epochs} | \"\n","                  f\"Train Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.2f}% | \"\n","                  f\"Val Loss: {avg_val_loss:.4f}, Acc: {val_accuracy:.2f}% | \"\n","                  f\"Time: {epoch_duration:.2f}s\")\n","\n","            early_stopper(val_accuracy, model)\n","            if early_stopper.early_stop:\n","                print(\"→ Early stopping triggered.\")\n","                break\n","\n","        total_training_time = time.time() - total_start_time\n","        print(f\"Total training time for config (lr={lr}, wd={wd}): {total_training_time:.2f}s\")\n","\n","        if early_stopper.best_score > best_val_acc:\n","            best_val_acc = early_stopper.best_score\n","            best_config = {'lr': lr, 'weight_decay': wd}\n","            best_model_state = model.state_dict()\n","\n","        # Free up memory after each run\n","        del model, optimizer, scheduler\n","        torch.cuda.empty_cache()\n","\n","# Save and report the best configuration\n","print(f\"Best Val Acc: {best_val_acc:.2f}% with config: {best_config}\")\n","torch.save(best_model_state, \"best_model.pt\")\n","print(\"Saved best model to best_model.pt\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"eo195zeBA8nw"},"outputs":[],"source":["# Load the best-performing model checkpoint, set to evaluation mode, and move it to the target device\n","model.load_state_dict(torch.load('best_model.pt'))\n","model.eval()\n","model.to(device)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Lad7Owz1A8nw"},"source":["Evaluation was based on average cross-entropy loss, overall accuracy, per-class metrics via classification report, and confusion matrix visualization using the test set. Note since the number of classes was very big confussion matrix was not really informative. It is basically gray square with white diagonal. This is why I did not report them in the actual report.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"XV_fL2kHA8nw"},"outputs":[],"source":["def evaluate_model(model, data_loader, criterion, device, class_names):\n","    model.eval()\n","    total_loss, correct, total = 0, 0, 0\n","    all_preds = []\n","    all_labels = []\n","\n","    print(\"→ Starting evaluation...\")\n","\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            total_loss += loss.item() * images.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            # Store predictions and ground truths for analysis\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    avg_loss = total_loss / total\n","    accuracy = 100 * correct / total\n","\n","    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n","    print(f\"Evaluation Accuracy: {accuracy:.2f}%\\n\")\n","\n","    # Detailed metrics per class\n","    print(\"→ Classification Report:\")\n","    print(classification_report(all_labels, all_preds, target_names=class_names))\n","\n","    # Confusion matrix visualization\n","    print(\"→ Generating Confusion Matrix...\")\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=class_names, yticklabels=class_names)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"editable":false,"trusted":true,"id":"HXqedppLA8nx"},"outputs":[],"source":["# Generate a list of class names in the format \"Class 0\", \"Class 1\", ..., based on the sorted unique label values in the dataset\n","class_names = [f\"Class {i}\" for i in sorted(labels_df['label'].unique())]"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"ahNyVD85A8nx"},"outputs":[],"source":["# call the evaluate model function\n","evaluate_model(model, test_loader, criterion, device, class_names)"]},{"cell_type":"markdown","metadata":{"id":"aDbqZf3NA8nx"},"source":["Plotting the curves for validational losses, accuracy and erros."]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"4WrT4re1A8n5"},"outputs":[],"source":["\n","# Plot losses\n","plt.figure(figsize=(12,5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Loss over Epochs')\n","plt.legend()\n","\n","# Plot accuracies\n","plt.subplot(1, 2, 2)\n","plt.plot(train_accuracies, label='Train Accuracy')\n","plt.plot(val_accuracies, label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy (%)')\n","plt.title('Accuracy over Epochs')\n","plt.legend()\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"Temvz_2iA8n5"},"outputs":[],"source":["val_errors = [100 - acc for acc in val_accuracies]\n","\n","plt.plot(val_errors, label='Validation Error')\n","plt.xlabel('Epoch')\n","plt.ylabel('Error (%)')\n","plt.title('Validation Error over Epochs')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"editable":false,"id":"Ph6avewEA8n5"},"source":["Time analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"kgZRzPZWA8n5"},"outputs":[],"source":["total_training_time = time.time() - total_start_time\n","print(f\"\\nTotal Training Time: {total_training_time / 60:.2f} minutes\")\n","print(f\"Average Epoch Time: {np.mean(epoch_times):.2f} seconds\")\n","print(f\"Fastest Epoch: {np.min(epoch_times):.2f} s | Slowest Epoch: {np.max(epoch_times):.2f} s\")"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"trusted":true,"id":"PZCU-A4oA8n5"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(range(1, len(epoch_times)+1), epoch_times, marker='o')\n","plt.title(\"Epoch Duration Over Time\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Time (seconds)\")\n","plt.grid(True)\n","plt.show()\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":401923,"sourceId":13663,"sourceType":"competition"}],"dockerImageVersionId":31040,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}